{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load 31_saving.py\n",
    "\n",
    "import urllib2\n",
    "# Un petit utilitaire fait maison pour écrire un CSV avec des Dictionaires et Unicode\n",
    "from goodiebag.writer import UnicodeWriter\n",
    "# Si BeautifulSoup n'est pas installé: pip install beautifulsoup4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base = \"https://www.leboncoin.fr/velos/offres/ile_de_france/paris/?location=Paris&o=\"\n",
    "ads = []\n",
    "\n",
    "def clean(string):\n",
    "    return string.strip().replace('\\n', ' ').replace('\\r', ' ')\n",
    "\n",
    "def save_ad(soup):\n",
    "    # On stock plusieurs valeurs dans un dictionaire nommé \"values\" et on ajoute\n",
    "    values = {\n",
    "        'title':       clean(soup.select('h1')[0].text),\n",
    "        'description': clean(soup.select('.line.properties_description')[0].text),\n",
    "        'price':       clean(soup.select('.item_price .value')[0].text),\n",
    "        'city':        clean(soup.select('.line_city .value')[0].text)\n",
    "    }\n",
    "    # On a ajoute cette annonce à la liste des annonces à sauvegarder\n",
    "    ads.append(values)\n",
    "\n",
    "def fetch_ad(url):\n",
    "    # On ouvre l'autre URL et on stock le resultat dans body\n",
    "    body = urllib2.urlopen(url).read()\n",
    "    # Parse le HTML avec Beautiful Soup\n",
    "    save_ad( BeautifulSoup(body, 'html.parser') )\n",
    "\n",
    "def fetch_list(page = 1):\n",
    "    # On ouvre l'autre URL et on stock le resultat dans body\n",
    "    body = urllib2.urlopen(base + str(page)).read()\n",
    "    # Parse le HTML avec Beautiful Soup\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    # Tous les éléments de la liste\n",
    "    for list_item in soup.select(\".list_item\"):\n",
    "        # Seulement les annonces avec un prix\n",
    "        if list_item.section.h3 != None:\n",
    "            url = list_item['href']\n",
    "            # L'URL sur Le Bon Coin ne contient pas le protocol\n",
    "            url = url if url.startswith(\"http\") else \"http:%s\" % url\n",
    "            # On affiche dans la console la procédure en cours\n",
    "            print u\"Téléchargement de %s\" % url\n",
    "            # On appelle ici l'autre fonction pour télécharger une annonce\n",
    "            fetch_ad(url)\n",
    "\n",
    "# Toutes les annonces de la page 1 sont stockées dans la liste \"ads\"\n",
    "fetch_list(1)\n",
    "# On affiche le resultat au format CSV\n",
    "writer = UnicodeWriter(open(\"./ads.csv\", 'w'), fieldnames=['title', 'price', 'city', 'description'])\n",
    "# Ajoute toutes les lignes une par une\n",
    "( writer.writerow(ad) for ad in ads  )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
